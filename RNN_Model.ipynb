{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.079391Z",
     "start_time": "2019-11-05T11:09:15.678564Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "def create_dataset(signal_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(signal_data)-look_back):\n",
    "        dataX.append(signal_data[i:(i+look_back), 0])\n",
    "        dataY.append(signal_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "class CustomHistory(keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "\n",
    "look_back = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.089363Z",
     "start_time": "2019-11-05T11:09:25.082383Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 데이터셋 생성하기\n",
    "signal_data = []\n",
    "for i in range(365):\n",
    "    random_5 = random.randint(1,5)\n",
    "    signal_data.append(random_5)\n",
    "signal_data=np.array(signal_data)\n",
    "signal_data=signal_data[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.102343Z",
     "start_time": "2019-11-05T11:09:25.093353Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "signal_data = scaler.fit_transform(signal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.113309Z",
     "start_time": "2019-11-05T11:09:25.107317Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 분리\n",
    "train = signal_data[0:220]\n",
    "val = signal_data[220:290]\n",
    "test = signal_data[290:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.127269Z",
     "start_time": "2019-11-05T11:09:25.116293Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "x_train, y_train = create_dataset(train, look_back)\n",
    "x_val, y_val = create_dataset(val, look_back)\n",
    "x_test, y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.140260Z",
     "start_time": "2019-11-05T11:09:25.129258Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터셋 전처리\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.733676Z",
     "start_time": "2019-11-05T11:09:25.143219Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "for i in range(2):\n",
    "    model.add(LSTM(32, batch_input_shape=(1, look_back, 1), stateful=True, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "model.add(LSTM(32, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:09:25.763559Z",
     "start_time": "2019-11-05T11:09:25.736631Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:21:05.570318Z",
     "start_time": "2019-11-05T11:09:25.765553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 4s 18ms/step - loss: 2.7469 - val_loss: 2.2174\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.2079 - val_loss: 2.2440\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1039 - val_loss: 2.2852\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1499 - val_loss: 2.2590\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1840 - val_loss: 2.2239\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1400 - val_loss: 2.2110\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1183 - val_loss: 2.2238\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1746 - val_loss: 2.1984\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0396 - val_loss: 2.2054\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1985 - val_loss: 2.2098\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1379 - val_loss: 2.2421\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1423 - val_loss: 2.1938\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1089 - val_loss: 2.2135\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1896 - val_loss: 2.2236\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1229 - val_loss: 2.1973\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0250 - val_loss: 2.1943\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 5s 25ms/step - loss: 2.1342 - val_loss: 2.2214\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 5s 22ms/step - loss: 2.2839 - val_loss: 2.2128\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 4s 17ms/step - loss: 2.1202 - val_loss: 2.1913\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 16ms/step - loss: 2.0604 - val_loss: 2.2173\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 14ms/step - loss: 2.0953 - val_loss: 2.1704\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 14ms/step - loss: 2.0606 - val_loss: 2.1844\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.9775 - val_loss: 2.1751\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 2.0631 - val_loss: 2.1848\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 2.0877 - val_loss: 2.2008\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0945 - val_loss: 2.1749\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0867 - val_loss: 2.2039\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0637 - val_loss: 2.1741\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0457 - val_loss: 2.1807\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.4984 - val_loss: 2.2213\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0182 - val_loss: 2.1869\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0640 - val_loss: 2.1905\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9765 - val_loss: 2.1720\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0923 - val_loss: 2.1831\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.2199 - val_loss: 2.1915\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0127 - val_loss: 2.1817\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0379 - val_loss: 2.1758\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1423 - val_loss: 2.1789\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0606 - val_loss: 2.1888\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9414 - val_loss: 2.1667\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1096 - val_loss: 2.1669\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 2.0019 - val_loss: 2.1702\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.9877 - val_loss: 2.1737\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 2.0862 - val_loss: 2.1770\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 2.1178 - val_loss: 2.1726\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 2.0776 - val_loss: 2.1825\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0137 - val_loss: 2.1827\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0742 - val_loss: 2.1708\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0677 - val_loss: 2.1858\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.9881 - val_loss: 2.1879\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0216 - val_loss: 2.1700\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9568 - val_loss: 2.1790\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0864 - val_loss: 2.1848\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9697 - val_loss: 2.1818\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9914 - val_loss: 2.1723\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0824 - val_loss: 2.1795\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0153 - val_loss: 2.1761\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0357 - val_loss: 2.1757\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9912 - val_loss: 2.1717\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1174 - val_loss: 2.1779\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0214 - val_loss: 2.1907\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0890 - val_loss: 2.1708\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0770 - val_loss: 2.1713\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1105 - val_loss: 2.1689\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0122 - val_loss: 2.1708\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0234 - val_loss: 2.1752\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9938 - val_loss: 2.1699\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9668 - val_loss: 2.1691\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1024 - val_loss: 2.1710\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8807 - val_loss: 2.1775\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1513 - val_loss: 2.1681\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0024 - val_loss: 2.1723\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1133 - val_loss: 2.1707\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0710 - val_loss: 2.1791\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 1.9941 - val_loss: 2.1711\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 2.0418 - val_loss: 2.1810\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0309 - val_loss: 2.1772\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 2.0628 - val_loss: 2.1770\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 2.1018 - val_loss: 2.1746\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0286 - val_loss: 2.1694\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.9936 - val_loss: 2.1688\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.1450 - val_loss: 2.1716\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1188 - val_loss: 2.1689\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9467 - val_loss: 2.1659\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9578 - val_loss: 2.1657\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0289 - val_loss: 2.1679\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0843 - val_loss: 2.1728\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9515 - val_loss: 2.1666\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0758 - val_loss: 2.1675\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.9259 - val_loss: 2.1688\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0254 - val_loss: 2.1692\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1326 - val_loss: 2.1730\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0676 - val_loss: 2.1681\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0455 - val_loss: 2.1660\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0539 - val_loss: 2.1705\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0273 - val_loss: 2.1736\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.1199 - val_loss: 2.1715\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 1.9374 - val_loss: 2.1679\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 2.0492 - val_loss: 2.1682\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 2.0481 - val_loss: 2.1655\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0767 - val_loss: 2.1678\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0696 - val_loss: 2.1678\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.9877 - val_loss: 2.1594\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 2.0903 - val_loss: 2.1546\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9569 - val_loss: 2.1824\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9808 - val_loss: 2.1581\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9503 - val_loss: 2.1493\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9399 - val_loss: 2.1635\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9068 - val_loss: 2.1611\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9511 - val_loss: 2.1825\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8491 - val_loss: 2.1887\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 1.9485 - val_loss: 2.1847\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 1.8954 - val_loss: 2.2187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.8626 - val_loss: 2.2212\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.8980 - val_loss: 2.2169\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.9365 - val_loss: 2.2326\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.8549 - val_loss: 2.3111\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0151 - val_loss: 2.2335\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 2.0541 - val_loss: 2.2690\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.7848 - val_loss: 2.2725\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9830 - val_loss: 2.2895\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8896 - val_loss: 2.2571\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.9221 - val_loss: 2.2953\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8404 - val_loss: 2.3046\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8919 - val_loss: 2.3430\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8288 - val_loss: 2.3498\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8513 - val_loss: 2.2610\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.7763 - val_loss: 2.2840\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8977 - val_loss: 2.2652\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.8756 - val_loss: 2.2738\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.7668 - val_loss: 2.2631\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.8368 - val_loss: 2.2695\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.7847 - val_loss: 2.2588\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 1.8217 - val_loss: 2.2651\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.8821 - val_loss: 2.2747\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 4s 17ms/step - loss: 1.8394 - val_loss: 2.3019\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 14ms/step - loss: 1.8391 - val_loss: 2.3171\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 209s 997ms/step - loss: 1.7117 - val_loss: 2.4193\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 9s 41ms/step - loss: 1.6825 - val_loss: 2.4079\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 6s 26ms/step - loss: 1.7083 - val_loss: 2.4203\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 16ms/step - loss: 1.6541 - val_loss: 2.4240\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 4s 17ms/step - loss: 1.7276 - val_loss: 2.3376\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 15ms/step - loss: 1.6375 - val_loss: 2.3956\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 1.7098 - val_loss: 2.4010\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 9s 41ms/step - loss: 1.5363 - val_loss: 2.3340\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 6s 27ms/step - loss: 1.5792 - val_loss: 2.2594\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 1.5592 - val_loss: 2.3240\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 4s 17ms/step - loss: 1.6907 - val_loss: 2.3698\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.6602 - val_loss: 2.2524\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.5493 - val_loss: 2.2865\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 1.5866 - val_loss: 2.2159\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.5485 - val_loss: 2.3402\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 13ms/step - loss: 1.6157 - val_loss: 2.5698\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 1.6192 - val_loss: 2.3666\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 1.5254 - val_loss: 2.4897\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.5524 - val_loss: 2.3939\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.4248 - val_loss: 2.3876\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.4522 - val_loss: 2.3662\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.5497 - val_loss: 2.4254\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.5432 - val_loss: 2.5989\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.4596 - val_loss: 2.4956\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.4866 - val_loss: 2.4468\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.4355 - val_loss: 2.4252\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.4437 - val_loss: 2.5825\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.2986 - val_loss: 2.5943\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.4848 - val_loss: 2.6153\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.5230 - val_loss: 2.7243\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.4796 - val_loss: 2.8646\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 14ms/step - loss: 1.5499 - val_loss: 2.5340\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210/210 [==============================] - 2s 12ms/step - loss: 1.4750 - val_loss: 2.7217\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3551 - val_loss: 2.5723\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3467 - val_loss: 2.6410\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.4146 - val_loss: 2.6680\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3676 - val_loss: 2.6012\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3783 - val_loss: 2.6824\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3126 - val_loss: 2.6946\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3698 - val_loss: 2.8881\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3786 - val_loss: 2.8749\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.3456 - val_loss: 2.8693\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 1.2667 - val_loss: 2.8246\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.3622 - val_loss: 2.6118\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.2265 - val_loss: 2.6625\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 3s 12ms/step - loss: 1.2134 - val_loss: 2.7803\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 12ms/step - loss: 1.2055 - val_loss: 2.9952\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 11ms/step - loss: 1.3787 - val_loss: 2.7171\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.3523 - val_loss: 2.7506\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.0855 - val_loss: 2.7398\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.2316 - val_loss: 2.6592\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.0687 - val_loss: 2.8457\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.1454 - val_loss: 2.8402\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 0.9955 - val_loss: 2.8093\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 9ms/step - loss: 1.1054 - val_loss: 3.0431\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.0803 - val_loss: 3.0764\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.1615 - val_loss: 3.1270\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.0577 - val_loss: 2.7022\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 0.9817 - val_loss: 2.7941\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 0.9883 - val_loss: 2.9848\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 1.1425 - val_loss: 2.7747\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 9ms/step - loss: 1.0034 - val_loss: 2.8438\n",
      "Train on 210 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 2s 10ms/step - loss: 0.9530 - val_loss: 2.8876\n"
     ]
    }
   ],
   "source": [
    "# 4. 모델 학습시키기\n",
    "custom_hist = CustomHistory()\n",
    "custom_hist.init()\n",
    "\n",
    "for i in range(200):\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=1, shuffle=False, callbacks=[custom_hist], validation_data=(x_val, y_val))\n",
    "    model.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:21:05.737900Z",
     "start_time": "2019-11-05T11:21:05.572310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. 학습과정 살펴보기\n",
    "plt.plot(custom_hist.train_loss)\n",
    "plt.plot(custom_hist.val_loss)\n",
    "plt.ylim(0.0, 0.15)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:21:06.144797Z",
     "start_time": "2019-11-05T11:21:05.739863Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 210 samples",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-826d8dbee16e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 6. 모델 평가하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Score: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainScore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvalScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1002\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                    steps=steps)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1766\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1769\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1498\u001b[0m                                  \u001b[1;34m'a number of samples that can be '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m                                  \u001b[1;34m'divided by the batch size. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m                                  str(x[0].shape[0]) + ' samples')\n\u001b[0m\u001b[0;32m   1501\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 210 samples"
     ]
    }
   ],
   "source": [
    "# 6. 모델 평가하기\n",
    "trainScore = model.evaluate(x_train, y_train, verbose=1)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "valScore = model.evaluate(x_val, y_val, verbose=0)\n",
    "model.reset_states()\n",
    "print('Validataion Score: ', valScore)\n",
    "testScore = model.evaluate(x_test, y_test, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:21:06.147790Z",
     "start_time": "2019-11-05T11:09:15.704Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. 모델 사용하기\n",
    "look_ahead = 10\n",
    "xhat = x_test[0]\n",
    "predictions = np.zeros((look_ahead,1))\n",
    "for i in range(look_ahead):\n",
    "    prediction = model.predict(np.array([xhat]), batch_size=1)\n",
    "    predictions[i] = prediction\n",
    "    xhat = np.vstack([xhat[1:],prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T11:21:06.149765Z",
     "start_time": "2019-11-05T11:09:15.706Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(np.arange(look_ahead),predictions,'r',label=\"prediction\")\n",
    "plt.plot(np.arange(look_ahead),y_test[:look_ahead],label=\"test function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
