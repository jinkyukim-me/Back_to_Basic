{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T15:18:30.946707Z",
     "start_time": "2019-11-05T14:19:29.748051Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 37s 49ms/step - loss: 0.0837 - val_loss: 0.0474\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 36s 47ms/step - loss: 0.0405 - val_loss: 0.0144\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 42s 55ms/step - loss: 0.0207 - val_loss: 0.0088\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 38s 50ms/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 38s 50ms/step - loss: 0.0158 - val_loss: 0.0069\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 43ms/step - loss: 0.0085 - val_loss: 0.0200\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0081 - val_loss: 0.0070\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0071 - val_loss: 0.0054\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0088 - val_loss: 0.0048\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.0059 - val_loss: 0.0134\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 40ms/step - loss: 0.0065 - val_loss: 0.0144\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 40ms/step - loss: 0.0065 - val_loss: 0.0054\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 34s 45ms/step - loss: 0.0194 - val_loss: 0.0433\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 34s 45ms/step - loss: 0.0174 - val_loss: 0.0048\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 36s 47ms/step - loss: 0.0064 - val_loss: 0.0242\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 43ms/step - loss: 0.0060 - val_loss: 0.0284\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 40ms/step - loss: 0.0068 - val_loss: 0.0265\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0940 - val_loss: 0.1284\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.1335 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 28s 37ms/step - loss: 0.1314 - val_loss: 0.1272\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.1311 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 40ms/step - loss: 0.1310 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.1137 - val_loss: 0.1282\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.1319 - val_loss: 0.1262\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 40ms/step - loss: 0.1283 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.1083 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 28s 37ms/step - loss: 0.1311 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.1309 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.1309 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 28s 37ms/step - loss: 0.1309 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 39ms/step - loss: 0.1309 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 39ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 40ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 39ms/step - loss: 0.1309 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 36s 48ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 40ms/step - loss: 0.1318 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 43ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 40ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 43ms/step - loss: 0.1309 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 40ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.1308 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.1309 - val_loss: 0.1273\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 34s 44ms/step - loss: 0.1312 - val_loss: 0.1269\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.1098 - val_loss: 0.0744\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0502 - val_loss: 0.0135\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 43ms/step - loss: 0.0264 - val_loss: 0.0194\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0196 - val_loss: 0.0180\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0158 - val_loss: 0.0126\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0180 - val_loss: 0.0099\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.0278 - val_loss: 0.0190\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0256 - val_loss: 0.0143\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0204 - val_loss: 0.0333\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0454 - val_loss: 0.0174\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/760 [==============================] - 29s 39ms/step - loss: 0.0144 - val_loss: 0.0040\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0230 - val_loss: 0.0278\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0111 - val_loss: 0.0063\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0134 - val_loss: 0.0189\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 34s 45ms/step - loss: 0.0205 - val_loss: 0.0032\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0084 - val_loss: 0.0032\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0073 - val_loss: 0.0059\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0074 - val_loss: 0.0013\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 39ms/step - loss: 0.0115 - val_loss: 0.0184\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0115 - val_loss: 0.0143\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 39ms/step - loss: 0.0121 - val_loss: 0.0026\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0070 - val_loss: 0.0017\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 34s 45ms/step - loss: 0.0094 - val_loss: 0.0108\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 43ms/step - loss: 0.0108 - val_loss: 0.0064\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 40ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 34s 45ms/step - loss: 0.0090 - val_loss: 0.0150\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 43ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0075 - val_loss: 0.0085\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0061 - val_loss: 0.0047\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 40s 53ms/step - loss: 0.0074 - val_loss: 0.0051\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0060 - val_loss: 0.0069\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0058 - val_loss: 0.0125\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 28s 37ms/step - loss: 0.0086 - val_loss: 0.0230\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 39ms/step - loss: 0.0056 - val_loss: 0.0022\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0068 - val_loss: 0.0241\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.0070 - val_loss: 0.0028\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0052 - val_loss: 0.0027\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0075 - val_loss: 0.0100\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 39ms/step - loss: 0.0072 - val_loss: 0.0043\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0052 - val_loss: 0.0011\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0080 - val_loss: 0.0030\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 43ms/step - loss: 0.0051 - val_loss: 0.0019\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.0060 - val_loss: 0.0077\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0058 - val_loss: 0.0043\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.0043 - val_loss: 0.0066\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 43ms/step - loss: 0.0063 - val_loss: 0.0111\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 43ms/step - loss: 0.0054 - val_loss: 0.0039\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 38s 50ms/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 33s 44ms/step - loss: 0.0054 - val_loss: 0.0019\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 34s 44ms/step - loss: 0.0053 - val_loss: 0.0028\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 35s 46ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 43ms/step - loss: 0.0048 - val_loss: 0.0011\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0044 - val_loss: 7.8451e-04\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0045 - val_loss: 0.0088\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0052 - val_loss: 0.0082\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 42s 55ms/step - loss: 0.0046 - val_loss: 7.6984e-04\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0050 - val_loss: 0.0036\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 43ms/step - loss: 0.0041 - val_loss: 0.0103\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.0036 - val_loss: 0.0053\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 32s 41ms/step - loss: 0.0049 - val_loss: 0.0090\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0051 - val_loss: 0.0123\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 31s 41ms/step - loss: 0.0063 - val_loss: 0.0018\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 30s 39ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Train on 760 samples, validate on 360 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/760 [=============>................] - ETA: 16s - loss: 0.0039"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d39b1493c1c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcustom_hist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def create_dataset(signal_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(signal_data)-look_back):\n",
    "        dataX.append(signal_data[i:(i+look_back), 0])\n",
    "        dataY.append(signal_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "class CustomHistory(keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "\n",
    "look_back = 40\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "signal_data = np.cos(np.arange(1600)*(20*np.pi/1000))[:,None]\n",
    "\n",
    "# 데이터 전처리\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "signal_data = scaler.fit_transform(signal_data)\n",
    "\n",
    "# 데이터 분리\n",
    "train = signal_data[0:800]\n",
    "val = signal_data[800:1200]\n",
    "test = signal_data[1200:]\n",
    "\n",
    "# 데이터셋 생성\n",
    "x_train, y_train = create_dataset(train, look_back)\n",
    "x_val, y_val = create_dataset(val, look_back)\n",
    "x_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "for i in range(2):\n",
    "    model.add(LSTM(32, batch_input_shape=(1, look_back, 1), stateful=True, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "model.add(LSTM(32, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "custom_hist = CustomHistory()\n",
    "custom_hist.init()\n",
    "\n",
    "for i in range(200):\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=1, shuffle=False, callbacks=[custom_hist], validation_data=(x_val, y_val))\n",
    "    model.reset_states()\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "plt.plot(custom_hist.train_loss)\n",
    "plt.plot(custom_hist.val_loss)\n",
    "plt.ylim(0.0, 0.15)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "trainScore = model.evaluate(x_train, y_train, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "valScore = model.evaluate(x_val, y_val, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Validataion Score: ', valScore)\n",
    "testScore = model.evaluate(x_test, y_test, batch_size=1, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "look_ahead = 250\n",
    "xhat = x_test[0]\n",
    "predictions = np.zeros((look_ahead,1))\n",
    "for i in range(look_ahead):\n",
    "    prediction = model.predict(np.array([xhat]), batch_size=1)\n",
    "    predictions[i] = prediction\n",
    "    xhat = np.vstack([xhat[1:],prediction])\n",
    "    \n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(np.arange(look_ahead),predictions,'r',label=\"prediction\")\n",
    "plt.plot(np.arange(look_ahead),y_test[:look_ahead],label=\"test function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
